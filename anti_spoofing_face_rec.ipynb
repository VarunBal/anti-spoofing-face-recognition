{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6665469a",
   "metadata": {},
   "source": [
    "# Anti spoofing Face authentication\n",
    "\n",
    "At the present age of technology, it is very important to protect your data and your devices, for which we use biometric authentication systems to verify the identity of the user before providing access. The most common example of these are fingerprints and facial recognition.\n",
    "\n",
    "Facial Recognition based authentication although very convinient has it drawbacks, as it can be easily fooled if not implemented properly.\n",
    "\n",
    "In this notebook, we will create a face authentication system that cannot be easily spoofed by showing an image of the person to unlock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed83b52",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56949ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import depthai as dai\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from face_auth import authenticate_face, enroll_face, delist_face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991abf3b",
   "metadata": {},
   "source": [
    "## Create the DepthAi Pipeline\n",
    "\n",
    "We create the DepthAi pipeline to get the depth map and the right camera output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c0802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - two mono (grayscale) cameras\n",
    "left = pipeline.createMonoCamera()\n",
    "left.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "left.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "\n",
    "right = pipeline.createMonoCamera()\n",
    "right.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "right.setBoardSocket(dai.CameraBoardSocket.RIGHT)\n",
    "\n",
    "# Create a node that will produce the depth map (using disparity output as it's easier to visualize depth this way)\n",
    "depth = pipeline.createStereoDepth()\n",
    "depth.setConfidenceThreshold(200)\n",
    "depth.setOutputRectified(True)  # The rectified streams are horizontally mirrored by default\n",
    "depth.setRectifyEdgeFillColor(0)  # Black, to better see the cutout\n",
    "depth.setExtendedDisparity(True)  # For better close range depth perception\n",
    "\n",
    "# Set the median filter to smooth out the depth map.\n",
    "median = dai.StereoDepthProperties.MedianFilter.KERNEL_7x7  # Options: MEDIAN_OFF, KERNEL_3x3, KERNEL_5x5, KERNEL_7x7 (default)\n",
    "depth.setMedianFilter(median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff5837",
   "metadata": {},
   "source": [
    "## Define Outputs For the Pipeline\n",
    "\n",
    "We link the output of the mono cameras to the input of the stereo depth node and define the rectified right and the depth outputs from the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd85d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "left.out.link(depth.left)\n",
    "right.out.link(depth.right)\n",
    "\n",
    "# Create Right output\n",
    "xout_right = pipeline.createXLinkOut()\n",
    "xout_right.setStreamName(\"right\")\n",
    "depth.rectifiedRight.link(xout_right.input)\n",
    "\n",
    "# Create depth output\n",
    "xout = pipeline.createXLinkOut()\n",
    "xout.setStreamName(\"disparity\")\n",
    "depth.disparity.link(xout.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aab487",
   "metadata": {},
   "source": [
    "## Helper Function \n",
    "\n",
    "We use the `overlay_symbol` function to display 'Lock' symbol on the output frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b829551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image of a lock in locked position\n",
    "locked_img = cv2.imread(os.path.join('data', 'images', 'lock_grey.png'), -1)\n",
    "# Load image of a lock in unlocked position\n",
    "unlocked_img = cv2.imread(os.path.join('data', 'images', 'lock_open_grey.png'), -1)\n",
    "\n",
    "\n",
    "def overlay_symbol(frame, img, pos=(65, 100)):\n",
    "    \"\"\"\n",
    "    This function overlays the image of lock/unlock\n",
    "    if the authentication of the input frame\n",
    "    is successful/failed.\n",
    "    \"\"\"\n",
    "    # Offset value for the image of the lock/unlock\n",
    "    symbol_x_offset = pos[0]\n",
    "    symbol_y_offset = pos[1]\n",
    "\n",
    "    # Find top left and bottom right coordinates\n",
    "    # where to place the lock/unlock image\n",
    "    y1, y2 = symbol_y_offset, symbol_y_offset + img.shape[0]\n",
    "    x1, x2 = symbol_x_offset, symbol_x_offset + img.shape[1]\n",
    "\n",
    "    # Scale down alpha channel between 0 and 1\n",
    "    mask = img[:, :, 3]/255.0\n",
    "    # Inverse of the alpha mask\n",
    "    inv_mask = 1-mask\n",
    "\n",
    "    # Iterate over the 3 channels - R, G and B\n",
    "    for c in range(0, 3):\n",
    "        # Add the lock/unlock image to the frame\n",
    "        frame[y1:y2, x1:x2, c] = (mask * img[:, :, c] +\n",
    "                                  inv_mask * frame[y1:y2, x1:x2, c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a96d79",
   "metadata": {},
   "source": [
    "## Initialize wls filter\n",
    "Here we initialize wls filter to use it in the next step for preprocessing the depth map and remove noise from it.\n",
    "\n",
    "You can see how well it does for removing noise and smoothing out the depth map.\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2021/06/without-wls-filter.jpg\"> | <img src=\"https://learnopencv.com/wp-content/uploads/2021/06/wls-filter.png\">\n",
    ":--:|:--:\n",
    "Without WLS Filter | With WLS Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbef2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wlsFilter\n",
    "wlsFilter = cv2.ximgproc.createDisparityWLSFilterGeneric(False)\n",
    "wlsFilter.setLambda(8000)\n",
    "wlsFilter.setSigmaColor(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315867a",
   "metadata": {},
   "source": [
    "## Load Cassification Model To Identify spoof\n",
    "\n",
    "We have trained a simple CNN binary classfier to classify between the real and spoofed depth maps.\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2021/06/real-faces.png\"> | <img src=\"https://learnopencv.com/wp-content/uploads/2021/06/spoofed-faces.png\">\n",
    ":--:|:--:\n",
    "Sample depth maps of Real faces | Sample depth maps of Spoofed faces\n",
    "\n",
    "For more info and to train your own classifier, refer to the model training notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "411d4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial spoofed classification model\n",
    "model_file = \"identify-spoof_with_ext_wls.25-0.99.h5\"\n",
    "model_input_size = (64, 64)\n",
    "detection_model = load_model(model_file, compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dd0f4e",
   "metadata": {},
   "source": [
    "## face_auth module\n",
    "\n",
    "We have created a `face_auth.py` module built using keras-facenet module to handle the face detection and authention. \n",
    "\n",
    "We use three functions from face_auth i.e `authenticate_face`, `enlist_face` and  `delist_face`.\n",
    "\n",
    "### `authenticate_face`\n",
    "\n",
    "Input: image_frame\n",
    "\n",
    "Returns: boolean (to indicate if the detected face is authenticated or not), bounding box for the detected face\n",
    "\n",
    "### enroll_face\n",
    "\n",
    "It takes the image as input and saves the face embedding for the detected face.\n",
    "\n",
    "Input: image_frame\n",
    "\n",
    "### delist_face\n",
    "\n",
    "It takes the image as input and removes the face embedding for the detected face.\n",
    "\n",
    "Input: image_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13efaad1",
   "metadata": {},
   "source": [
    "## Main Loop To Get Frames And Perform Authentication\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2021/06/anti-spoofing-face-rec_flowchart.png\">\n",
    "\n",
    "First we get both the right and the depth frame from the output streams.\n",
    "\n",
    "We apply the wls filter on the depth map to remove most of the noise in the depth map.\n",
    "\n",
    "Then we pass the right camera frame to the `authenticate_face` function which will return the bounding box for the face and an boolean value indicating that if the face is authenticated or not.\n",
    "\n",
    "Once we have the Bounding box we can use it to get the region of face from the depth map and feed it through the previously trained classifer to check if the face is a real face or a spoofed one.\n",
    "\n",
    "Once we have verified that the face is real we can use the `enroll_face` function to enroll the face and face its embedding. Similarily we can use the `delist_face` function to remove an already enrolled face.\n",
    "\n",
    "Finally we display all the info onto the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d7b0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected.\n",
      "No face detected.\n",
      "No face detected.\n",
      "No face detected.\n",
      "real\n",
      "real\n",
      "real\n",
      "real\n",
      "real\n",
      "real\n",
      "real\n",
      "real\n",
      "real\n"
     ]
    }
   ],
   "source": [
    "# Frame count\n",
    "count = 0\n",
    "\n",
    "# Set the number of frames to skip\n",
    "SKIP_FRAMES = 10\n",
    "\n",
    "# Pipeline defined, now the device is connected to\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Start pipeline\n",
    "    device.startPipeline()\n",
    "\n",
    "    # Output queue will be used to get the rectified right frames from the outputs defined above\n",
    "    q_right = device.getOutputQueue(name=\"right\", maxSize=4, blocking=False)\n",
    "\n",
    "    # Output queue will be used to get the disparity frames from the outputs defined above\n",
    "    q_depth = device.getOutputQueue(name=\"disparity\", maxSize=4, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        # Get right camera frame\n",
    "        in_right = q_right.get()\n",
    "        r_frame = in_right.getFrame()\n",
    "        r_frame = cv2.flip(r_frame, flipCode=1)\n",
    "        # cv2.imshow(\"right\", r_frame)\n",
    "\n",
    "        # Get depth frame\n",
    "        in_depth = q_depth.get()  # blocking call, will wait until a new data has arrived\n",
    "        depth_frame = in_depth.getFrame()\n",
    "        # depth_frame = (depth_frame*multiplier).astype(np.uint8)\n",
    "        depth_frame = np.ascontiguousarray(depth_frame)\n",
    "        depth_frame = cv2.bitwise_not(depth_frame)\n",
    "\n",
    "        # Apply wls filter\n",
    "        # cv2.imshow(\"without wls filter\", cv2.applyColorMap(depth_frame, cv2.COLORMAP_JET))\n",
    "        depth_frame = wlsFilter.filter(depth_frame, r_frame)\n",
    "\n",
    "        # frame is transformed, the color map will be applied to highlight the depth info\n",
    "        depth_frame_cmap = cv2.applyColorMap(depth_frame, cv2.COLORMAP_JET)\n",
    "        # frame is ready to be shown\n",
    "        cv2.imshow(\"disparity\", depth_frame_cmap)\n",
    "\n",
    "        # Retrieve 'bgr' (opencv format) frame from gray scale\n",
    "        frame = cv2.cvtColor(r_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        if count % SKIP_FRAMES == 0:\n",
    "            # Authenticate the face present in the frame\n",
    "            authenticated, bbox = authenticate_face(frame)\n",
    "\n",
    "        # Set default status\n",
    "        status_color = (0, 0, 255)\n",
    "        status = 'No Face Detected.'\n",
    "        unlock = False\n",
    "\n",
    "        # Check if a face was detected in the frame\n",
    "        if bbox:\n",
    "            # Get face roi from right and depth frames\n",
    "            face_d = depth_frame[max(0, bbox[1]):bbox[1] + bbox[3], max(0, bbox[0]):bbox[0] + bbox[2]]\n",
    "            face_r = r_frame[max(0, bbox[1]):bbox[1] + bbox[3], max(0, bbox[0]):bbox[0] + bbox[2]]\n",
    "            cv2.imshow(\"face_roi\", face_d)\n",
    "\n",
    "            # Preprocess face depth map for classification\n",
    "            resized_face_d = cv2.resize(face_d, model_input_size)\n",
    "            resized_face_d = resized_face_d / 255\n",
    "            resized_face_d = np.expand_dims(resized_face_d, axis=-1)\n",
    "            resized_face_d = np.expand_dims(resized_face_d, axis=0)\n",
    "\n",
    "            # Get prediction\n",
    "            result = detection_model.predict(resized_face_d)\n",
    "            if result[0][0] > .5:\n",
    "                prediction = 'spoofed'\n",
    "                is_real = False\n",
    "            else:\n",
    "                prediction = 'real'\n",
    "                is_real = True\n",
    "            print(prediction)\n",
    "\n",
    "            # Check if face is real\n",
    "            if is_real:\n",
    "                # Check if the face in the frame was authenticated\n",
    "                if authenticated:\n",
    "                    # Set Status\n",
    "                    status_color = (0, 255, 0)\n",
    "                    status = 'Authenticated'\n",
    "                    unlock = True\n",
    "                else:\n",
    "                    # Set Status\n",
    "                    status = 'Unauthenticated'\n",
    "            else:\n",
    "                # Set Status\n",
    "                status = 'Unauthenticated'\n",
    "                # Display \"Spoof detected\" status on the bbox\n",
    "                cv2.putText(frame, 'Spoof Detected', (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))\n",
    "\n",
    "        # Display bounding box\n",
    "        cv2.rectangle(frame, bbox, status_color, 2)\n",
    "\n",
    "        # Create background for showing details\n",
    "        cv2.rectangle(frame, (5, 5, 175, 150), (50, 0, 0), -1)\n",
    "\n",
    "        # Display authentication status on the frame\n",
    "        cv2.putText(frame, status, (20, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color)\n",
    "\n",
    "        # Display lock symbol\n",
    "        if unlock:\n",
    "            overlay_symbol(frame, unlocked_img)\n",
    "        else:\n",
    "            overlay_symbol(frame, locked_img)\n",
    "\n",
    "        # Display instructions on the frame\n",
    "        cv2.putText(frame, 'Press E to Enroll Face.', (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255))\n",
    "        cv2.putText(frame, 'Press D to Delist Face.', (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255))\n",
    "        cv2.putText(frame, 'Press Q to Quit.', (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255))\n",
    "\n",
    "        # Capture the key pressed\n",
    "        key_pressed = cv2.waitKey(1) & 0xff\n",
    "\n",
    "        # Enrol the face if e was pressed\n",
    "        if key_pressed == ord('e'):\n",
    "            if is_real:\n",
    "                enroll_face([frame])\n",
    "        # Delist the face if d was pressed\n",
    "        elif key_pressed == ord('d'):\n",
    "            if is_real:\n",
    "                delist_face([frame])\n",
    "        # Stop the program if q was pressed\n",
    "        elif key_pressed == ord('q'):\n",
    "            break\n",
    "\n",
    "        # Display the final frame\n",
    "        cv2.imshow(\"Authentication Cam\", frame)\n",
    "\n",
    "        # Increment frame count\n",
    "        count += 1\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8680fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
